{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sagi-brudni/ML-HW1/blob/main/Ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Ex1 - Unsupervised learning**"
   ],
   "metadata": {
    "id": "djcj7z3isEoG",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Names and IDs\n",
    "\n",
    "1.   Sagi Brudni (315481010)\n",
    "2.   Sapir Anidgar (322229410)\n"
   ],
   "metadata": {
    "id": "bbLvYfOMnaOr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this assignemnt you will practice unsupervised methods we saw in class, and specifically running K-means and visualizing the data using PCA.\n",
    "\n",
    "In this assignment you will learn a few more things:\n",
    "\n",
    "1.   Load local files\n",
    "2.   Load data from Kaggle\n",
    "3. Use Scikit-learn K-means\n",
    "4. Use Scikit-learn PCA\n",
    "5. Some visulaization\n",
    "6. Evaluate the performance of the clustering using Elbow methods, Siouhette analysis and accuracy (as we have true labels)\n",
    "\n",
    "Note:\n",
    "* Read the complete task before implementing.\n",
    "* Reuse code, write functions."
   ],
   "metadata": {
    "id": "vpmlOOaKWUHr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import All Packages\n",
    "Add all imports needed for this notebook to run"
   ],
   "metadata": {
    "id": "SmSdJm9coKpE",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement here\n",
    "!pip install keras\n",
    "!pip install google-colab\n",
    "!pip install tensorflow\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib.colors import to_rgba\n",
    "from keras.datasets import mnist"
   ],
   "metadata": {
    "id": "ZAw8GGhYT1gJ",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Clustering And Dimension Reduction"
   ],
   "metadata": {
    "id": "y_-HIZ7Rnkfa",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise you will cluster fake news using `k-means` and visualize the clustering using PCA."
   ],
   "metadata": {
    "id": "kQKN1XWKtGu2",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Import the Fake News Dataset from Kaggle**\n",
    "\n",
    "Navigate to https://www.kaggle.com. Then go to the [Account tab of your user profile](https://www.kaggle.com/me/account) and select Create API Token. This will trigger the download of kaggle.json, a file containing your API credentials.\n",
    "\n",
    "Then run the cell below and click the upload button to upload kaggle.json to your Colab runtime.\n",
    "\n",
    "After uploading the kaggle.json the fake news dataset will be copy to the enviroment in the '/content' directory. You will see the two files 'Fake.csv' and 'True.csv'.\n",
    "\n",
    "For more about the dataset you can read [here](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)."
   ],
   "metadata": {
    "id": "cjvrUWutYAQD",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "# upload kaggle.json file using user prompt\n",
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "  \n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# download the dataset\n",
    "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "# extract the files\n",
    "!unzip '/content/fake-and-real-news-dataset.zip'"
   ],
   "metadata": {
    "id": "pt0gDVr_YGvc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "outputId": "10e6b597-e3fb-4baf-b521-67d22ef228be",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Read the csv files and create one single dataframe (5 points)\n",
    "\n",
    "*   Create a dataframe which is the join of the two files 'Fake.csv' and 'True.csv'.\n",
    "*   Extract the 'text' column from each dataframe (droping title, subject and date columns).\n",
    "*   Create a single dataframe containing a text column and a label column (fake=0, real=1)."
   ],
   "metadata": {
    "id": "sIT7GQbLo9-c",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement here\n",
    "# Example for reading one file. It will work if the previous step was successful.\n",
    "fake_df = pd.read_csv('/content/Fake.csv')\n",
    "true_df = pd.read_csv('/content/True.csv')\n",
    "\n",
    "# Extract the 'text' column from each dataframe\n",
    "fake_text_df = fake_df.filter([\"text\"])\n",
    "true_text_df = true_df.filter([\"text\"])\n",
    "\n",
    "# Add label column\n",
    "fake_text_df[\"label\"] = 0\n",
    "true_text_df[\"label\"] = 1\n",
    "\n",
    "# Create dataframe of fake and true, with text and label as columns\n",
    "joint_df = pd.concat([fake_text_df, true_text_df], ignore_index=True)\n",
    "\n",
    "joint_df"
   ],
   "metadata": {
    "id": "tYvY12rep0sO",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "f50cd8a4-0c1d-49dd-98d7-e2f417ce7a27",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Cluster the samples into 2 clusters (real and fake) (15 points)\n",
    "\n",
    "*   Generate TF-IDF features by applying the TfidfVectorizer preprocessor using 1000 features (`max_features=1000`) and remove English stop words.\n",
    "* Scale the data.\n",
    "*    Apply k-Means algorithm on the TF-IDF features using n_clusters=2."
   ],
   "metadata": {
    "id": "-Wp5qcYwqBxi",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement here\n",
    "# Extract features from text and remove stop words\n",
    "def generate_tf_idf_features(text_df):\n",
    "  \"\"\"\n",
    "  This method extracts features from vector of texts\n",
    "  :param text_df: pandas column of texts\n",
    "  :return: pandas dataframe that has 1000 features and the values are calculated using TF-IDF\n",
    "  \"\"\"\n",
    "  vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "  vectors = vectorizer.fit_transform(text_df)\n",
    "\n",
    "  # Turn features into dataframe\n",
    "  feature_names = vectorizer.get_feature_names()\n",
    "  dense = vectors.todense()\n",
    "  denselist = dense.tolist()\n",
    "  return pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "tf_idf_df = generate_tf_idf_features(joint_df[\"text\"])\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(tf_idf_df)\n",
    "\n",
    "# apply k-Means with 2 clusters on the scaled data\n",
    "k_means_alg = KMeans(n_clusters=2)\n",
    "k_means_alg.fit(scaled_df)"
   ],
   "metadata": {
    "id": "gIpApLQvqB-J",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8997f73e-b215-4fec-c19b-0c9336f761d3",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Visualize using scatter plot (15 points)\n",
    "The data is high dimensional, so for visualization purpose, we will perform a dimensionality reduction using PCA.\n",
    "\n",
    "* Apply PCA\n",
    "*   Visualize the clustering in 2d using first two PCs.\n",
    "*   Visualize the clustering in 3d using first three PCs.\n",
    "\n",
    "Notes:\n",
    "\n",
    "*   In clustering visualization it's important to also visualize the centroids.\n",
    "* Visualize using matplotlib scatter function. It can be used to plot 2D or 3D scatter plots.\n",
    "* Color the points according to the true labels."
   ],
   "metadata": {
    "id": "d4hK3vpTtRTT",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement here\n",
    "possible_colors = [\"#FFFF00\", \"#FF0000\", '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "def visualize_data(real_labels, df, centroids, clusters_number):\n",
    "  \"\"\"\n",
    "  This method creates 3D and 2D graphs of the data\n",
    "  :param real_labels: vector that represents the real label of each sample\n",
    "  :param df: the dataframe to visualize\n",
    "  :param centroids: the centroids that the model produced\n",
    "  :param clusters_number:\n",
    "  :return:\n",
    "  \"\"\"\n",
    "  # Transpose matrix for scatter function - we get a list for each dimension\n",
    "  transposed_df = df.T\n",
    "  # Transpose centroids for the same reason\n",
    "  transposed_centroids = centroids.T\n",
    "\n",
    "  # Define colors for each cluster \n",
    "  colors = [to_rgba(possible_colors[k], 0.25) for k in range(clusters_number) for i in real_labels if i == k]\n",
    "\n",
    "  # 3D visualization\n",
    "  fig = plt.figure(figsize=(12, 12))\n",
    "  ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "  ax.scatter(*transposed_df[:3], c=colors, alpha=0.035)\n",
    "  ax.scatter(*transposed_centroids[:3], c=\"black\", s=400)\n",
    "  plt.show()\n",
    "\n",
    "  # 2D visualization - use only the first 2 dimensions\n",
    "  plt.scatter(*transposed_df[:2], c=colors)\n",
    "  plt.scatter(*transposed_centroids[:2], c=\"black\", s=400)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# For any k, using PCA with any i > k components will lead to the same first k PCs\n",
    "# This is why we can use PCA with n_components=3 and use the first two PCs for 2D visualiztion\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Transform data into the reduced dimension\n",
    "pca_df = pca.fit_transform(scaled_df)\n",
    "\n",
    "# Transform centroids\n",
    "pca_centroids = pca.transform(k_means_alg.cluster_centers_)\n",
    "\n",
    "visualize_data(joint_df[\"label\"], pca_df, pca_centroids, 2)"
   ],
   "metadata": {
    "id": "DUhtdsvGtlJu",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984
    },
    "outputId": "78582d0c-1b69-429b-86fc-063287720213",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Performance evaluation (15 points)\n",
    "### How did our clustering perform against the real labels?\n",
    "\n",
    "We do have the true lables (fake/real), but we don't know which cluster correspond to which label. Therefore, we check the two options:\n",
    "\n",
    "*   Define cluster 1 as fake and cluster 2 as real. What is the accuracy?\n",
    "*   Define cluster 1 as true and cluster 2 as fake. What is the accuracy?"
   ],
   "metadata": {
    "id": "ELEMd4WFFTwS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement here\n",
    "\n",
    "# Calculate manually \n",
    "def calc_accuracy(real_labels, predicted_labels):\n",
    "  \"\"\"\n",
    "  :param real_labels: vector that represents the real label of each sample\n",
    "  :param predicted_labels: vector that represents the predicted label the model per each sample\n",
    "  :return: accuracy score\n",
    "  \"\"\"\n",
    "  count = 0\n",
    "  for i in range(len(predicted_labels)):\n",
    "    if predicted_labels[i] == real_labels[i]: # correct prediction\n",
    "      count += 1\n",
    "\n",
    "  return count / len(predicted_labels) # correct / total\n",
    "\n",
    "# since calc_accuracy compares to the real labels (fake=0 and true=1), the \n",
    "# first cluster(0) is considered as fake and the second(1) is considered as true\n",
    "print(\"our accuracy score, first cluster=fake, second cluster=true:\",\n",
    "      calc_accuracy(joint_df[\"label\"], k_means_alg.labels_))\n",
    "# flip clusters - first cluster (0) is considered as true and second cluster (1) is considered as fake\n",
    "print(\"our accuracy score, first cluster=true, second cluster=fake:\",\n",
    "      calc_accuracy(joint_df[\"label\"], 1 - k_means_alg.labels_))\n",
    "\n",
    "# Alternatively, we can use sklearn accuracy_score function\n",
    "print(\"sklearn accuracy score, first cluster=fake, second cluster=true:\", \n",
    "      accuracy_score(joint_df[\"label\"], k_means_alg.labels_))\n",
    "print(\"sklearn accuracy score, first cluster=true, second cluster=false:\",\n",
    "      accuracy_score(joint_df[\"label\"], 1 - k_means_alg.labels_))\n",
    "\n"
   ],
   "metadata": {
    "id": "hVVyjDIoJKeu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f1b2b5ea-4d9d-4518-c5c6-9c87f2a7593f",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Find optimal `k` (10 points)\n",
    "What is the best k for clustering?\n",
    "\n",
    "* Cluster using a range of `k` (up to 20) and compare the SSD and the Silhouette values for every k.\n",
    "* Plot SSD vs. k and Silhouette score vs. k.\n",
    "\n",
    "Notes:\n",
    "* You can get the SSD of a clustering using the `inertia_` attribute of the model.\n",
    "* Silhouette score using `silhouette_score` function from `sklearn.metrics`. This function accept the model and the data.\n",
    "* Computing Silhouette may takes long time. Estimate the Silhouette using a sample of 300 samples uisng the argument `sample_size=300`."
   ],
   "metadata": {
    "id": "Go3Jx05Wcsr4",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Implement here\n",
    "def find_optimal_k(df, first_k, last_k, silhouette_sample_size):\n",
    "  k_values = list(range(first_k, last_k))\n",
    "  silhouette_list = []\n",
    "  ssd_list = []\n",
    "\n",
    "  # cluster using k from 2 to 20 (not inclusive)\n",
    "  for k in k_values:\n",
    "    k_means_alg = KMeans(n_clusters=k)\n",
    "    k_means_alg.fit(df)\n",
    "\n",
    "    ssd_list.append(k_means_alg.inertia_)\n",
    "    silhouette_list.append(silhouette_score(df, k_means_alg.labels_, sample_size=silhouette_sample_size))\n",
    "\n",
    "  # Plot SSD vs. k\n",
    "  plt.plot(k_values, ssd_list)\n",
    "  plt.xlabel('k') \n",
    "  plt.ylabel('SSD') \n",
    "  plt.title(\"SSD vs. k\")\n",
    "  plt.show()\n",
    "\n",
    "  # Plot Silhouette score vs. k\n",
    "  plt.plot(k_values, silhouette_list)\n",
    "  plt.xlabel('k') \n",
    "  plt.ylabel('Silhouette score') \n",
    "  plt.title(\"Silhouette score vs. k\")\n",
    "  plt.show()\n",
    "\n",
    "\"\"\"k_values = list(range(2, 20))\n",
    "silhouette_list = []\n",
    "ssd_list = []\n",
    "\n",
    "# cluster using k from 2 to 20 (not inclusive)\n",
    "for k in k_values:\n",
    "  k_means_alg = KMeans(n_clusters=k)\n",
    "  k_means_alg.fit(scaled_df)\n",
    "\n",
    "  ssd_list.append(k_means_alg.inertia_)\n",
    "  silhouette_list.append(silhouette_score(scaled_df, k_means_alg.labels_, sample_size=300))\n",
    "\n",
    "# Plot SSD vs. k\n",
    "plt.plot(k_values, ssd_list)\n",
    "plt.xlabel('k') \n",
    "plt.ylabel('SSD') \n",
    "plt.title(\"SSD vs. k\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette score vs. k\n",
    "plt.plot(k_values, silhouette_list)\n",
    "plt.xlabel('k') \n",
    "plt.ylabel('Silhouette score') \n",
    "plt.title(\"Silhouette score vs. k\")\n",
    "plt.show()\"\"\"\n",
    "\n",
    "find_optimal_k(scaled_df, 2, 20, 300)"
   ],
   "metadata": {
    "id": "DvGLxRHGWPop",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "outputId": "e7b17010-53d7-459d-eec6-3c0b20c0352e",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. What is the optimal k for clustering? (10 points)\n",
    "\n",
    "* Explain.\n",
    "* If optimal k!=2 what can be a good explanation for this?"
   ],
   "metadata": {
    "id": "7WcPTIareSCz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimal k for clustering is 2.\n",
    "as said in the lecture, for each sample:\n",
    "\n",
    "ai = average distance from the sample to all data points in the same cluster\n",
    "\n",
    "bi = average distance from the sample to all data points in the closest\n",
    "cluster\n",
    "\n",
    "s(i) = (bi - ai) / max(bi, ai)\n",
    "\n",
    "Hence: high Silhouette score (close to 1) means that the distance from the sample to other samples in that cluster is small, and the distance from the sample to samples in other clusters is high. This is good clustering. Low Silhouette score (close to -1) means that the samples are closer to other clusters than to their clusters (bad clustering). \n",
    "\n",
    "So, we want to achieve the highest Silhouette score, and therefore we will choose k=2 as can be seen from the graph Silhouette score vs. k above\n",
    "\n",
    "In addition, we cannot see a convincing \"elbow\" shape in the SSD vs. k graph. k = 2 sounds like a reasonable choice, because this is the first possible option for choosing k. \n",
    "\n"
   ],
   "metadata": {
    "id": "k3s2biB6uyVg",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. PCA then k-means (10 points)\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n",
    "\n",
    "This time, lets try to change the order.\n",
    "1. Generate TF-IDF 1000 features\n",
    "2. Run PCA (using all dimenssions)\n",
    "3. Run k-means\n",
    "4. Plot 2D and 3D scatter plots\n",
    "5. Estimate the accuracy according to true labels.\n",
    "6. Explain how is it compared to the previous approach, where clustering is performed w/o PCA.\n",
    "\n",
    "Note:\n",
    "* When you compute the PCA, don't limit to first PCs, but use all of them."
   ],
   "metadata": {
    "id": "g1Cuo2LNR3R-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tf_idf_df = generate_tf_idf_features(joint_df[\"text\"])\n",
    "\n",
    "pca = PCA()\n",
    "transformed_data = pca.fit_transform(tf_idf_df)\n",
    "\n",
    "# apply k-Means with 2 clusters on the scaled data\n",
    "k_means_alg = KMeans(n_clusters=2)\n",
    "k_means_alg.fit(transformed_data)\n",
    "\n",
    "pca_centroids = pca.transform(k_means_alg.cluster_centers_) # Transform centroids\n",
    "\n",
    "visualize_data(joint_df[\"label\"], transformed_data, pca_centroids, 2)\n",
    "\n",
    "# first cluster(0) is considered as fake and the second(1) is considered as true\n",
    "print(\"first cluster=fake, second cluster=true:\",\n",
    "      calc_accuracy(joint_df[\"label\"], k_means_alg.labels_))\n",
    "# flip clusters - first cluster (0) is considered as true and second cluster (1) is considered as fake\n",
    "print(\"first cluster=true, second cluster=fake:\",\n",
    "      calc_accuracy(joint_df[\"label\"], 1 - k_means_alg.labels_))"
   ],
   "metadata": {
    "id": "NJf98Fx9WCce",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "21569b26-a475-440f-9831-281f0e251f21",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MNIST clustering (10 points)\n",
    "Redo the same analysis, but this time using the MNIST data set. Notice that this time there are actually 10 'true' clusters.\n",
    "\n",
    "1. What is the accuracy of the clustering when using k=10? \\\\\n",
    "   * When for computing the accuracy of 10 classes, first you need to define what is the label of each cluster. Do that by majority votes. In theory, you may get two or more clusters with the same labels. We will ignore that for now. Meaning that you don't need to bother in case there are two clusters with the same label according to the majority vote. Simply count what is the fraction of 'other' digits in each cluster.\n",
    "2. What is the optimal k?\n",
    "3. How do the results changes if you first run PCA?"
   ],
   "metadata": {
    "id": "JiYp9sP0apoz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# notice that we don't need to scale the data beacause all the features values are 0 or 1.\n",
    "\n",
    "def load_mnist_data():\n",
    "  \"\"\"\n",
    "  The function load the mnist dataset to np.array and combining the train set and test set into one set.\n",
    "  :return: X, y - all the features and their labels.\n",
    "  \"\"\"\n",
    "  (train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "  X = np.concatenate((train_X, test_X), axis=0)\n",
    "  y = np.concatenate((train_y, test_y), axis=0)\n",
    "\n",
    "  # change each sample to one vector instead of matrix\n",
    "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "  return X,y\n",
    "\n",
    "\n",
    "def label_clusters_by_majority_of_votes(y, clustering_labels):\n",
    "  \"\"\"\n",
    "  The function computes the label of the clusters that the KMeans algorithm returned, based on the majority vote of the true label of each sample in the cluster.\n",
    "  :param y: The true labels of the dataset\n",
    "  :param clustering_labels: The labels of the clusters that the KMeans algorithm returned.\n",
    "  :return: array of the labels given to the KMeans clusters by majority vote of the samples in each cluster\n",
    "  \"\"\"\n",
    "  k_means_labels = np.zeros(y.shape, dtype=np.uint8)\n",
    "  for i in range(10):\n",
    "    # get indecis of the current cluster\n",
    "    indices_current_cluster = np.where(clustering_labels == i)\n",
    "    # choose majority of votes of real labels to define the cluster label\n",
    "    majority_of_votes = np.bincount(y[indices_current_cluster]).argmax()\n",
    "    # save chosen label for each sample in the cluster\n",
    "    k_means_labels[indices_current_cluster] = majority_of_votes\n",
    "  return k_means_labels\n",
    "\n",
    "\n",
    "### 1) Calculate accuracy when PCA performed after KMeans ###\n",
    "X,y = load_mnist_data()\n",
    "\n",
    "# apply k-Means with 10 clusters\n",
    "k_means = KMeans(n_clusters=10)\n",
    "k_means.fit(X)\n",
    "k_means_labels = label_clusters_by_majority_of_votes(y, k_means.labels_)\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA()\n",
    "# Transform data into the reduced dimension\n",
    "pca_df = pca.fit_transform(X)\n",
    "# Transform centroids - SHOULD CHANGE COLOR\n",
    "pca_centroids = pca.transform(k_means.cluster_centers_)\n",
    "visualize_data(y, pca_df, pca_centroids, 10)\n",
    "\n",
    "print(\"The accuracy score with PCA after:\")\n",
    "print(calc_accuracy(y, k_means_labels))\n",
    "\n",
    "### 2) The optimal K ###\n",
    "find_optimal_k(X, 2, 20, 300)\n",
    "\n",
    "### 3) Calculate accuracy when PCA performed before KMeans ###\n",
    "X,y = load_mnist_data()\n",
    "\n",
    "# apply PCA\n",
    "pca = PCA()\n",
    "# Transform data into the reduced dimension\n",
    "pca_df = pca.fit_transform(X)\n",
    "\n",
    "# apply k-Means with 10 clusters\n",
    "k_means = KMeans(n_clusters=10)\n",
    "k_means.fit(X)\n",
    "k_means_labels = label_clusters_by_majority_of_votes(y, k_means.labels_)\n",
    "# Transform centroids - SHOULD CHANGE COLOR\n",
    "pca_centroids = pca.transform(k_means.cluster_centers_)\n",
    "visualize_data(y, pca_df, pca_centroids, 10)\n",
    "\n",
    "print(\"The accuracy score with PCA before:\")\n",
    "print(calc_accuracy(y, k_means_labels))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Oxqq55k0k8k3",
    "outputId": "4e453e15-106d-40a7-94e0-2feb6c98eb34",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "1) The accuracy score of the clustering (with PCA after the KMeans algorithm) when using k = 10 is 0.5850285714285715.\n",
    "\n",
    "2) Based on the \"elbow\" method the optimal k is k=5. We can see from the SSD vs. k graph that the oprimal K is 5, because before k=5 the slope is steep and after k=5 it is moderate.\n",
    "However, based on the silhouette graph, k=2 is the optimal k. k=2 has the highest silhouette score, meaning that the distance from a sample to other samples in each cluster is small, and the distance from a sample to samples in other clusters is high. This is good clustering.\n",
    "\n",
    "3) The accuracy score of the clustering (with PCA before the KMeans algorithm) when using k = 10 is 0.5807428571428571."
   ],
   "metadata": {
    "id": "2tShcOPLxa2t",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute projection (10 points)\n",
    "**Notice:** No code required in this section.\n",
    "\n",
    "Given the next PCA projection matrix\n",
    "$\\begin{pmatrix}\n",
    "1 & 1\\\\\n",
    "2 & -1\n",
    "\\end{pmatrix}$\n",
    "And the correspoding egienvalues\n",
    "$(5, -1)$\n",
    "\n",
    "Compute the projection to one dimension of the next two vectors:\n",
    "\\begin{pmatrix}\n",
    "1 & 3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "3 & 7\n",
    "\\end{pmatrix}"
   ],
   "metadata": {
    "id": "3ft1JUt39-4x",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Answer here and show your computations (no code)"
   ],
   "metadata": {
    "id": "VCQQ0Jzxapo6",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The given projection matrix W is composed of 2 egienvectors.\n",
    "In order to project the vectors into one dimension, we should take the egienvector that has the corresponding highest egienvalue, which is\n",
    "$\\begin{pmatrix}\n",
    "1\\\\\n",
    "2\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "Project $\\begin{pmatrix}\n",
    "1 & 3\n",
    "\\end{pmatrix}$:\n",
    "\n",
    " $\\begin{pmatrix}\n",
    "1 & 2\n",
    "\\end{pmatrix}$ * \n",
    " $\\begin{pmatrix}\n",
    "1 \\\\ 3\n",
    "\\end{pmatrix}$ = 7\n",
    "\n",
    "Project $\\begin{pmatrix}\n",
    "3 & 7\n",
    "\\end{pmatrix}$:\n",
    "\n",
    " $\\begin{pmatrix}\n",
    "1 & 2\n",
    "\\end{pmatrix}$ * \n",
    " $\\begin{pmatrix}\n",
    "3 \\\\ 7\n",
    "\\end{pmatrix}$ = 17"
   ],
   "metadata": {
    "id": "Fzv4mYUkvqMB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}
